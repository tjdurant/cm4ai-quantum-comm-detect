{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CM4AI Community Detection Tutorial\n",
    "\n",
    "In graph theory, terms like \"communities,\" \"clusters,\" \"groups,\" and \"modules\" are often used interchangeably to describe subsets of nodes that are densely connected internally and sparsely connected to the rest of the network. This concept is central to community detection, which aims to identify such structures within graphs. Detecting these communities is crucial for understanding the organization and function of complex networks, as they often correspond to functional units within the system. \n",
    "\n",
    "Various algorithms have been developed to detect communities in networks. For instance, the Girvan–Newman algorithm identifies edges that lie between communities and removes them to reveal the community structure. Another popular method is modularity maximization, which searches for divisions of a network that have a high modularity score, indicating a strong community structure.\n",
    "\n",
    "In this tutorial we will walk through how to perform community detection in a simple graph with three communities.\n",
    "\n",
    "Along the way, we will learn about different metrics that are commonly used in Network Analysis which are related to this task. \n",
    "\n",
    "[Run In Google Colab](https://colab.research.google.com/github/tjdurant/cm4ai-quantum-comm-detect/blob/main/src/comm-detect-tutorial.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import community as community_louvain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Basic Graph with Three Communities\n",
    "\n",
    "Let's start by drawing a simple graph and visually/subjectively deciding how many 'communities' we think there are. And for the sake of simplicity, lets make the decision to analyze this graph in a way where each node is assigned to a single, distinct community (non-ovelapping community detection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_toy_graph():\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from([[1,2], [2,3], [1,3], [1,4], [1,5], [3,5],\n",
    "                  [6,7], [7,8], [8,9], [6,9], [6,8], [7,9],\n",
    "                  [10,11], [11,12], [10,12],\n",
    "                  [9,12], [2,7]])\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = create_toy_graph()\n",
    "nx.draw(G, with_labels=True, node_color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most might argue there are three communities here (although some might argue that 4 is it's own community). \n",
    "\n",
    "Let's say there are <u>three</u> 'communities' in this graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how do we arrive at 'three communities' using a mathematical/algorithmic process?\n",
    "\n",
    "Let's go through one method called the <u>Girvan-Newman Algorithm</u>. \n",
    "\n",
    "The first step is to calculate some metrics that help characterize the edges in this network. \n",
    "\n",
    "This is often done by using the general concept of centrality, which aims to quantify the “importance” of vertices or edges within a graph.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Edge Betweenness Centrality (EBC)</u> \n",
    "\n",
    "EBC asks the question, for every pair of nodes in the network, how many of the shortest paths between those nodes pass through a given edge. Stated a second way, it is a measure of the importance of an edge in a graph, calculated as the number of shortest paths between pairs of nodes that pass through that edge. It helps identify critical connections in networks.\n",
    "\n",
    "Intuitively, an edge with high betweenness is \"important\" because it frequently appears in shortest paths connecting different vertices in the graph.\n",
    "\n",
    "So, if we calculate for all edges, which would be the most important do you think?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_centrality_G = nx.edge_betweenness_centrality(G)\n",
    "\n",
    "sorted_by_values = dict(sorted(edge_centrality_G.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "for edge, centrality in sorted_by_values.items():\n",
    "    print(f\"{edge}: {centrality}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, [2,7] is the most 'important'\n",
    "\n",
    "Let's manually calculate edge betweenness for [2,7].\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u> Manually Calculate Edge Betweenness</u>\n",
    "\n",
    "Remember, EBC is a measure of how many shortest paths (between all pairs of vertices) go through a particular edge. \n",
    "\n",
    "$$ b(e) = \\sum_{s < t} \\frac{\\sigma_{s,t}(e)}{\\sigma_{s,t}} $$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\sigma_{s,t}(e)$ is the number of those shortest paths that pass through the edge $e$.\n",
    "- $\\sigma_{s,t}$ denotes the total number of shortest paths between vertices $s$ and $t$.\n",
    "- The summation $\\sum_{s < t}$ is taken over all pairs of distinct vertices $s$ and $t$. \n",
    "\n",
    "For undirected graphs, it is common to sum over $s < t$ (instead of $s \\neq t$) to avoid double-counting paths. The $s < t$ loops over every pair of distinct nodes in the graph exactly once rather than summing over both $(s,t)$ and $(t,s)$. This is purely a notational convenience for an undirected graph and ensures that every node pair is included **exactly once**, avoiding double-counting.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "So what would $\\sigma_{8,1}({2,7})$ be in the graph below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G3 = nx.Graph()\n",
    "G3.add_edges_from([\n",
    "    (1, 2), (1, 3),   # Connect 1 -> {2, 3}\n",
    "    (2, 7), (3, 7),   # Connect {2,3} -> 7\n",
    "    (7, 5), (7, 6), (7, 9),  # Connect 7 -> {5,6,9}\n",
    "    (5, 8), (6, 8), (9, 8)   # Connect {5,6,9} -> 8\n",
    "])\n",
    "nx.draw(G3, with_labels=True, node_color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all shortest paths from 1 to 8 have length 4, and exactly 6 such paths exist.\n",
    "\n",
    "Hence, $\\sigma_{1,8} = 6$\n",
    "\n",
    "When we then consider the number of 'shortest paths' from 1 to 8 that pass through the edge [2,7], this would be $\\sigma_{1,8}({2,7}) = 3$, giving us a 3/6 fraction for the (1,8) node pair.\n",
    "\n",
    "Then we do this for all pairs of nodes, and sum all of these fractions with the $s < t$ constraint. \n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the originally proposed graph we can use some shortcuts.\n",
    "\n",
    "Calculate the total number of possible node pairs in this network. Given that there are 12 nodes and we're looking for pairs (i.e., 2), we can use the '12 choose 2' calculation, simplified here:\n",
    "\n",
    "  $$ \\frac{12!}{2!(12-2)!} = 66 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pairs = (12*11)/2\n",
    "total_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then find the shortest paths for every pair and determine\n",
    "# for every 'shortest path' which go through [2,7].\n",
    "# For this graph we can calculate this via a shortcut by multiplying \n",
    "# by the number of nodes on either side of the [2,7] edge.\n",
    "number_of_nodes_passing_through = 5*7\n",
    "number_of_nodes_passing_through\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, 35 pairs of nodes in this network, rely on [2,7] on their shortest path to each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we divide these two numbers we get the edge betweenness\n",
    "print(f\"Edge Betweenness Centrality for edge [2, 7] in G by nx: {round(edge_centrality_G.get((2,7)),4)}\")\n",
    "print(f\"Edge Betweenness Centrality for edge [2, 7] in G by manual: {round((number_of_nodes_passing_through / total_pairs),4)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, [2,7] is the most 'important' as measured by this metric. \n",
    "\n",
    "But as a thought exercise, how could we decrease the centrality of that edge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a path for 'the dangler' over to node 2 that would make it's shortest path not use the [2,7] edge\n",
    "G2 = nx.Graph()\n",
    "G2.add_edges_from([[1,2], [2,3], [1,3], [1,4], [1,5], [3,5],\n",
    "                  [6,7], [7,8], [8,9], [6,9], [6,8], [7,9],\n",
    "                  [10,11], [11,12], [10,12],\n",
    "                  [9,12], [2,7], [4,7]])\n",
    "nx.draw(G2, with_labels=True, node_color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have added an edge between node 4 (i.e., 'the dangler') and node 7. In this network, the shortest path from 4 to 7 is no longer through the [2,7] edge. So the [2,7] calculation for edge betweenness becomes this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_centrality_G2 = nx.edge_betweenness_centrality(G2)\n",
    "print(f\"Edge Betweenness Centrality for edge [2, 7] in G: {round(edge_centrality_G.get((2,7)),4)}\")\n",
    "print(f\"Edge Betweenness Centrality for edge [2, 7] in G2: {round(edge_centrality_G2.get((2,7)),4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Girvan-Newman Algorithm</u>\n",
    "\n",
    "Now that we have the EBC for all of the edges, how does the Girvan-Newman Algorithm use that to find communities?\n",
    "\n",
    "Look for the edge that has the highest EBC and delete that edge. \n",
    "\n",
    "Recompute the EBC, and repeat. \n",
    "\n",
    "This is liberating communities from the network, so that hopefully at the end we have all of the communities by themselves. \n",
    "\n",
    "The trick is to know how many 'deletions' to do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 2\n",
    "\n",
    "G = create_toy_graph()\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    tmp_ebc = nx.edge_betweenness_centrality(G).items()\n",
    "    edge_to_delete = sorted(tmp_ebc, key=lambda pair: -pair[1])[0][0]\n",
    "    \n",
    "    G.remove_edge(*edge_to_delete)\n",
    "    \n",
    "    nx.draw(G, with_labels=True, node_color='r')\n",
    "    plt.title('Step %s\\nEdge %s Deleted'%(i, edge_to_delete), fontsize=20)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Girvan-Newman Community Detection ---\n",
    "# girvan_newman returns an iterator of partitions (each partition is a tuple of sets of nodes).\n",
    "# The first `next(...)` will give the first split (2 communities). \n",
    "# You can keep calling `next(...)` to get more finely split communities.\n",
    "G = create_toy_graph()\n",
    "\n",
    "gn_partitions = nx.community.girvan_newman(G)\n",
    "first_level_partition = next(gn_partitions)   # This is the partition with the first split\n",
    "num_communities_gn = len(first_level_partition)\n",
    "print(\"Number of communities (Girvan-Newman, first split):\", num_communities_gn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the crux of using Girvan-Newman is deciding when to stop splitting up the graph. \n",
    "\n",
    "You typically track a quality measure (often modularity) at each step and pick the partition that yields the best score (e.g., the highest modularity). In other words, you have to decide the “optimal cut” or watch when the modularity peaks.\n",
    "\n",
    "However, it can be computationally expensive for large networks, and you need to determine the best stopping point manually (by checking the modularity changes after each removal, or following some heuristic).\n",
    "\n",
    "There are some algorithms where the decision of **when to stop** is baked into the algorithm and in a way that optimizes performance. \n",
    "\n",
    "But, before we do that, we need to understand another metric commonly used in Network Analysis: **<u>Modularity</u>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modularity (Q)\n",
    "\n",
    "### Intuitive Understanding of Modularity\n",
    "\n",
    "Measures how well a graph is partitioned into communities. \n",
    "\n",
    "$Q \\propto \\sum_{s \\in S} \\Big[ (\\text{number of edges within group } s) \\;-\\; (\\text{number of EXPECTED edges within group } s) \\Big]$\n",
    "\n",
    "Intuitively, $Q$ is proporational to the summation over all the communities where for every community, we ask how many edges are there between the members of the group $s$, minus how many edges would I expect between the nodes of group $s$ in some random null model. \n",
    "\n",
    "And if the group $s$ connections are more than what we would expect in a random, then we have found a significant cluster.\n",
    "\n",
    "The total modularity of the network is the sum of the modularity score for each individual cluster/community.\n",
    "\n",
    "So, we need to figure a way to calculate the number of 'expected connections' within a community.\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Model\n",
    "\n",
    "The Configuration Model is a random graph model that preserves the degree distribution of the original graph. \n",
    "\n",
    "It generates a network by randomly connecting nodes while ensuring that each node's degree matches its specified value, thereby maintaining the overall <u>degree distribution</u> of the network. This model provides a more realistic baseline for expected edges in networks with heterogeneous degree distributions.\n",
    "\n",
    "The expected number of edges between two nodes $i $ and $j$ in the Configuration Model is given by:\n",
    "\n",
    "$\\text{Expected edges between } i \\text{ and } j = \\frac{k_i k_j}{2m}$\n",
    "\n",
    "where:\n",
    "- $k_i$ and $k_j$ are the degrees of nodes $i$ and $j$,\n",
    "- $2m$ is the total number of edges in the network.\n",
    "\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modularity Equation Breakdown\n",
    "\n",
    "$Q(G, S) = \\frac{1}{2m} \\sum_{s \\in S} \\sum_{i \\in s} \\sum_{j \\in s} \\left( A_{ij} - \\frac{k_i k_j}{2m} \\right)$\n",
    "\n",
    "Modularity a set of groups $S$ in graph $G$ is a sum over all the paris of nodes in the group, where we look at $A_{ij}$ to determine if that pair of nodes is connected in the group and then for every pair we calculate the expected number of edges between a pair of nodes using the Configuration Model as our null model. So we are saying what is the real number of edges, minus the expected number of edges, for all the pairs of nodes within a given group $s$, and now i sum this over all the groups $s$ into $S$. And $A_{ij}$ is 1 but if connection is 0. \n",
    "\n",
    "$\\frac{1}{2m}$ is a normalizing constant that keeps $Q$ in the range of -1 to 1.\n",
    "- Where if the modularity $Q$ for a given set of partitions $S$ is \"high\" (i.e., >0.5), the graph has significant community structure\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is Higher Modularity \"Better\"?\n",
    "\n",
    "Modularity measures how well nodes within a single community connect to each other relative to how they connect to nodes outside that community. A higher modularity for a given community means:\n",
    "\n",
    "- More internal cohesion: Most edges are between the nodes inside the community.\n",
    "- Less external linkage: There are fewer edges connecting those nodes to other parts of the network.\n",
    "\n",
    "If one finds that the summation of modularities is *\"high\"* across a set of partitions $S$ across a given graph $G$, the algorithm has likely done a good job at identifiying self-contained, distinct, or meaningful clusters within the larger network. \n",
    "\n",
    "The overall idea being that we have done a good job of identifing communities if we maximize modularity\n",
    "\n",
    "So then this would become the target of an objective function that we use formulate as a kind of optimization problem for community detection - i.e., we will try to maximize $Q(G,S)$, and to do this we will use the <u>Louvain Algorithm</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Louvain Algorithm\n",
    "\n",
    "The **Louvain algorithm** is a popular method for detecting communities with high modularity scores $Q$ in large networks due to its **speed** and **scalability**. \n",
    "\n",
    "- *Greedy* algorithm for community detection\n",
    "  - Refers to a strategy where decisions are made step-by-step, choosing the locally optimal solution at each step in the hope that this leads to a globally optimal solution.\n",
    "- Supports weighted graphs\n",
    "- Supports hierarchical clustering\n",
    "\n",
    "<br>\n",
    "\n",
    "It operates in two major phases that repeat iteratively:\n",
    "\n",
    "1. **Local Modularity Optimization**  \n",
    "   - Start with each node $i$ in its own community.  \n",
    "   - For each node $i$, consider moving it to the community of one of its neighbors.  \n",
    "   - Calculate the change in modularity $\\Delta Q$ for this move. If $\\Delta Q$ is **positive**, move node $i$ to that neighbor’s community.\n",
    "\n",
    "2. **Community Aggregation**  \n",
    "   - Once no more improvements can be made by local moves, **aggregate** each community into a “super-node.”  \n",
    "   - Replace each entire community with a single super-node, and build a smaller, coarser network.  \n",
    "   - Repeat Phase 1 on this aggregated network until no further modularity improvement is possible.\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = create_toy_graph()\n",
    "\n",
    "# --- Louvain Community Detection ---\n",
    "# best_partition returns a dict of node -> community ID\n",
    "louvain_partition = community_louvain.best_partition(G, resolution=1)\n",
    "num_communities_louvain = len(set(louvain_partition.values()))\n",
    "print(\"Number of communities (Louvain):\", num_communities_louvain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Modularity and the Gain\n",
    "\n",
    "In Louvain, the **modularity** $Q$ of a partition is commonly defined as:\n",
    "\n",
    "$$\n",
    "Q = \\frac{1}{2m} \\sum_{i,j} \\biggl(A_{ij} - \\frac{k_i \\, k_j}{2m}\\biggr) \\delta(c_i, c_j),\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $A_{ij}$ is the adjacency matrix (1 if there is an edge between $i$ and $j$, or the weight if weighted).  \n",
    "- $k_i$ is the degree (or sum of edge weights) of node $i$.  \n",
    "- $m$ is the total number of edges (or total edge weight).  \n",
    "- $\\delta(c_i, c_j)$ is 1 if $i$ and $j$ are in the same community, and 0 otherwise.\n",
    "\n",
    "When **moving** a single node $i$ from its current community to a neighboring community $C$, the **change in modularity** $\\Delta Q$ can be computed (in one simplified form) as:\n",
    "\n",
    "$$\n",
    "\\Delta Q \\approx \\frac{k_i^{\\text{in},C} - k_i \\frac{S_C}{2m}}{2m},\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $k_i^{\\text{in},C}$ is the sum of the weights of edges from node $i$ to the nodes in community $C$.  \n",
    "- $k_i$ is the sum of the weights of edges from node $i$ to **all** other nodes.  \n",
    "- $S_C$ is the sum of the degrees (edge weights) of the nodes in community $C$.  \n",
    "- $m$ is the total edge weight in the network.\n",
    "\n",
    "If $\\Delta Q > 0$, moving $i$ to $C$ **increases** overall modularity.\n",
    "\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Partitioning Using Quantum Annealing\n",
    "\n",
    "Following the methods described in [Negre et al.](https://www.osti.gov/pages/biblio/1628969), this process begins with creating an...\n",
    "\n",
    "### <u>Adjacency Matrix</u>\n",
    "\n",
    "$$\n",
    "A_{ij} =\n",
    "\\begin{cases} \n",
    "0, & \\text{if } i = j \\\\ \n",
    "w_{ij}, & \\text{if } i \\neq j \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "1. $A_{ij}$ represents the element in the $i$-th row and $j$-th column of the matrix.\n",
    "2. **Case 1:** If $i = j$, $A_{ij} = 0$. This means there are no self-loops in the graph, as the weight of an edge from a node to itself is zero.\n",
    "3. **Case 2:** If $i \\neq j$, $A_{ij} = w_{ij}$. Here, $w_{ij}$ is the weight of the edge connecting nodes $i$ and $j$. If no edge exists between $i$ and $j$, $w_{ij}$ is typically assumed to be zero.\n",
    "\n",
    "Because our toy graph is unweighted, if $i \\neq j$, $A_{ij} = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = create_toy_graph()\n",
    "\n",
    "A = nx.adjacency_matrix(G)\n",
    "print ('\\nAdjacency matrix:\\n', A.todense())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate... \n",
    "\n",
    "### <u>Node Degrees</u>\n",
    "\n",
    "This begins with calculating the number of **degrees** per node. A degree is 'how many edges are touching a node'. \n",
    "\n",
    "Given by:\n",
    "\n",
    "$k_i = \\sum_{j} A_{ij}$\n",
    "\n",
    "Where:\n",
    "- $k_i$: Degree of node $i$\n",
    "- $A_{ij}$: Element of the adjacency matrix representing the edge between nodes $i$ and $j$. So, here, we are summing the number of edges - which in an unweighted graph, each edge is equal to 1 - between $i$ and everyother node in the graph $j$. \n",
    "\n",
    "Below, you can see that node 0 is connected to 4 other nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_degrees = A.sum(axis=1)\n",
    "\n",
    "print(f\"{A.todense()}\\n\")\n",
    "print(node_degrees.T)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see above walk through some node examples:\n",
    "\n",
    "$k_0 = 4$\n",
    "\n",
    "$k_1 = 3$\n",
    "\n",
    "And so on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate the node degrees over the whole graph, for every node. \n",
    "\n",
    "This is defined as:\n",
    "\n",
    "$2m = \\sum_i k_i = \\sum_{ij} A_{ij}$\n",
    "\n",
    "1. $k_i$: The degree of node $i$, which is the sum of weights of edges connected to $i$.\n",
    "2. $\\sum_i k_i$: The sum of all node degrees, which is twice the total edge weight for undirected graphs.\n",
    "4. $\\sum_{ij} A_{ij}$: An equivalent way of summing over all edges in the adjacency matrix $A$, counting each edge's weight twice for undirected graphs.\n",
    "\n",
    "So, we can just use `.sum()` on `node_degrees` ($k_i$) to get node degree across whole graph ($2m$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_m = node_degrees.sum()\n",
    "two_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Note that $m$ (little) is the total number of edges (or the total edge weight in the case of weighted graphs) in the graph\n",
    "\n",
    "The total number of *node degrees* is $2m$\n",
    "\n",
    "In an undirected graph, each edge is shared between two nodes. When summing the degrees ($k_i$) of all nodes, each edge is counted twice, once for each endpoint.\n",
    "\n",
    "Mathematically this is stated as:\n",
    "\n",
    "$$\\sum_i k_i = 2m$$\n",
    "\n",
    "This is sometimes called the <u>**handshaking lemma**</u> in graph theory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the total number of edges ($m$) is half of this number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtotal = two_m / 2.0\n",
    "print(\"Sanity Check\")\n",
    "print(f\"Total number of edges calculated from node_degrees (2m):    {int(mtotal)}\")\n",
    "print(f\"Total number of edges calculated on the graph by nx:        {len(G.edges)} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the...\n",
    "\n",
    "### <u>**Modularity Matrix**</u>\n",
    "\n",
    "This is done by subtracting the adjacency matrix ($A_{ij}$) from the null model ($\\frac{k_i k_j}{2m}$).\n",
    "\n",
    "$\n",
    "B_{ij} = A_{ij} - \\frac{k_i k_j}{2m}\n",
    "$\n",
    "\n",
    "Lets calculate the *null model* first with generating a $k_i$ and $k_j$ matrix. \n",
    "\n",
    "The outer product of two vectors $\\mathbf{k}$ and $\\mathbf{k}$ is defined as:\n",
    "\n",
    "$\n",
    "\\text{Outer product: } (\\mathbf{k} \\otimes \\mathbf{k})_{ij} = k_i \\cdot k_j\n",
    "$\n",
    "\n",
    "In Python, this is computed using `np.outer()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that k_j is:\n",
    "node_degrees.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, to get the node_degree_matrix that is k_i * k_j, we can use np.outer():\n",
    "np.outer(node_degrees, node_degrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we put it together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modularity_matrix = A - (np.outer(node_degrees, node_degrees) / two_m)\n",
    "\n",
    "print (\"\\nModularity matrix: \\n\", modularity_matrix)\n",
    "\n",
    "print (\"min value = \", modularity_matrix.min())\n",
    "print (\"max value = \", modularity_matrix.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is known that modularity-based optimization of graph partitioning can fail to detect communities that are small ([Fortunato 2007](https://www.pnas.org/doi/abs/10.1073/pnas.0605965104))\n",
    "\n",
    "To help with this, we can add a 'resolution' parameter to our modularity matrix..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 3\n",
    "modularity_matrix = A - (resolution * np.outer(node_degrees, node_degrees) / two_m)\n",
    "\n",
    "print (\"\\nModularity matrix: \\n\", modularity_matrix)\n",
    "\n",
    "print (\"min value = \", modularity_matrix.min())\n",
    "print (\"max value = \", modularity_matrix.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher resolution ($resolution > 1$): Amplifies the importance of modularity differences, encouraging smaller, finer-grained communities.\n",
    "\n",
    "Lower resolution ($resolution < 1$):Reduces the weight of modularity differences, favoring larger communities.\n",
    "\n",
    "#### Resolution in the Louvain Algorithm\n",
    "\n",
    "In the Louvain algorithm:\n",
    "\n",
    "- The **resolution parameter** modifies the modularity optimization process to control the size of the detected communities.\n",
    "\n",
    "- A higher resolution parameter decreases the weight of the term $\\frac{k_i \\cdot k_j}{2m}$ (expected edges between nodes based on their degrees) relative to the actual edge weights. This encourages splitting large communities into smaller ones.\n",
    "\n",
    "- The modified modularity function becomes:\n",
    "\n",
    "$$\n",
    "Q = \\sum_{i,j} \\left[ w_{ij} - \\gamma \\frac{k_i \\cdot k_j}{2m} \\right] \\delta(c_i, c_j)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $w_{ij}$: edge weight between nodes $i$ and $j$,  \n",
    "- $k_i$: degree of node $i$,  \n",
    "- $2m$: sum of all edge weights in the graph,  \n",
    "- $\\gamma$: resolution parameter (default is 1),  \n",
    "- $\\delta(c_i, c_j)$: Kronecker delta, 1 if $i$ and $j$ are in the same community, 0 otherwise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "------\n",
    "\n",
    "So now that we have our Modularity Matrix ($B_{ij}$) we have to create the QUBO, or a...\n",
    "\n",
    "### Quadratic Unconstrained Binary Optimization formulation\n",
    "\n",
    "A QUBO is a mathematical formulation used in optimization problems. It expresses a problem as minimizing or maximizing a quadratic polynomial where the variables are binary (0 or 1). This formulation is often used in quantum computing, particularly for quantum annealers like D-Wave, because they solve problems expressed in this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you don't have ground truth, and modularity is suboptimal in small community space, how do you measure performance or how do you know you are detecting the right communtiies. \n",
    "\n",
    "Which ones consume the most QPU time\n",
    "\n",
    "How do you evaluate through a graph where you DON\"T know the number of partitions beforehand - could just leave n-parts = n-nodes\n",
    "\n",
    "could test hierarchical clustering with louvain \n",
    "\n",
    "Were there some hyperparameters taking up more time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why Is Louvain Faster Than Girvan–Newman?\n",
    "\n",
    "1. **Local, Greedy Optimizations**  \n",
    "   - Louvain updates communities by checking small, local moves for each node $i$.  \n",
    "   - Girvan–Newman iteratively **removes the highest-betweenness edge** and recalculates global betweenness each time, which is computationally expensive.\n",
    "\n",
    "2. **Hierarchical (Multi-Level) Approach**  \n",
    "   - After no more local moves can improve modularity, Louvain **collapses** each community into a super-node and repeats the process on a smaller graph.  \n",
    "   - This reduction significantly speeds up subsequent iterations.\n",
    "\n",
    "3. **No Repeated Global Betweenness Computation**  \n",
    "   - Girvan–Newman requires recomputing **edge betweenness** multiple times, which is costly ($O(n \\times m)$ or worse).  \n",
    "   - Louvain only needs to compute **local gains** in modularity ($\\Delta Q$) when moving nodes.\n",
    "\n",
    "4. **Early Termination**  \n",
    "   - Louvain naturally **stops** once modularity cannot be improved any further.  \n",
    "   - No need for exhaustive searches or repeated cuts of the graph.\n",
    "\n",
    "As a result, **Louvain’s greedy, multi-level collapsing** approach scales well to large networks, making it **significantly faster** in practice than Girvan–Newman for community detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cm4ai-quantum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
