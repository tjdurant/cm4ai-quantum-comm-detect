{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import numba\n",
    "import pandas as pd\n",
    "from community import community_louvain\n",
    "from dwave.cloud import Client\n",
    "from dwave.cloud.config import get_configfile_paths\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "from sklearn.metrics import adjusted_mutual_info_score, adjusted_rand_score\n",
    "\n",
    "import algorithm.kcomm.graph_kClusterAlgorithm_functions as QCD\n",
    "import algorithm.kcomm.graph_kClusterAlgorithm_functions_optimized as QCD_optimized\n",
    "import algorithm.kcomm.graphFileUtility_functions as GFU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwave_config_paths = get_configfile_paths(only_existing=False)\n",
    "\n",
    "token_found = False\n",
    "for config_path in dwave_config_paths:\n",
    "    try:\n",
    "        # Read the configuration file\n",
    "        config = configparser.ConfigParser()\n",
    "        config.read(config_path)\n",
    "\n",
    "        # Check if the 'defaults' section and 'token' exist\n",
    "        if 'defaults' in config and 'token' in config['defaults']:\n",
    "            token = config['defaults']['token']\n",
    "\n",
    "            # Check if the token is non-empty\n",
    "            if token.strip():\n",
    "                os.environ['DWAVE_API_TOKEN'] = token\n",
    "                print(f\"Set DWAVE_API_TOKEN from {config_path}\")\n",
    "                token_found = True\n",
    "                break  # Exit the loop once a valid token is found\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {config_path}: {e}\")\n",
    "\n",
    "# If no valid token was found, print a message\n",
    "if not token_found:\n",
    "    print(\"You need a valid D-Wave config file with a non-empty token.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and authenticate the Kaggle API\n",
    "api = KaggleApi()\n",
    "api.authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data\"\n",
    "competition = 'cm4ai-community-detection-benchmark'\n",
    "data_path = os.path.join(data_dir, competition)\n",
    "os.makedirs(data_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"../output\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all files from a competition (e.g., Titanic)\n",
    "competition = 'cm4ai-community-detection-benchmark'\n",
    "api.competition_download_files(competition, path=data_dir, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all files from the zip to the specified directory\n",
    "zip_file_path = data_path + \".zip\"\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {\n",
    "    \"benchmark_gen\":\"protein\",\n",
    "    \"output_dir\" : output_dir,\n",
    "    \"beta0\": 25,                     # A weight on a node's modularity \n",
    "    \"gamma0\": -500,                  # Penalty for nodes being placed in multiple communities\n",
    "    \"threshold\": 0.8,                # Unknown\n",
    "    \"qsize\": 64,                     # unknown\n",
    "    \"community_penalty_factor\": 1,   # Penalty factor for large communities\n",
    "    \"resolution\": 3                 # Higher modifies the modularity matrix to emphasize smaller communities\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_profile=\"defaults\"\n",
    "\n",
    "gt_arr = []\n",
    "if args_dict[\"benchmark_gen\"] == 'karate':\n",
    "\n",
    "    print(f\"Using benchmark graph generated by: nx-karate\")  \n",
    "    \n",
    "    run_label = \"zacharys-karate-club\"\n",
    "    input_graph = \"zacharys-karate-club\"\n",
    "    \n",
    "    G = nx.karate_club_graph() \n",
    "\n",
    "\n",
    "    gt_arr = [G.nodes[v]['club'] for v in G.nodes()]\n",
    "    gt_arr = [0 if x == 'Mr. Hi' else 1 for x in gt_arr]        # Convert to binary labels\n",
    "\n",
    "\n",
    "elif args_dict[\"benchmark_gen\"] == 'networkx':\n",
    "    \n",
    "    print(f\"Using benchmark graph generated by: networkx\")    \n",
    "\n",
    "    run_label = \"LFR_rs11_N1000_ad5_mc20_mu0.1\"\n",
    "    input_graph = f\"../data/cm4ai-community-detection-benchmark/{run_label}\"\n",
    "\n",
    "    G = nx.read_edgelist(f\"{input_graph}.edgelist\")\n",
    "\n",
    "    df = pd.read_csv(f\"{input_graph}_communities.csv\")\n",
    "    gt_dict = df.set_index('id')['solution'].to_dict()\n",
    "\n",
    "    sorted_by_keys = dict(sorted(gt_dict.items()))\n",
    "    gt_arr = []\n",
    "    for k,v in sorted_by_keys.items():\n",
    "        gt_arr.append(v)\n",
    "\n",
    "elif args_dict[\"benchmark_gen\"] == 'dynbench':\n",
    "\n",
    "    print(f\"Using benchmark graph generated by: Dynbench\")    \n",
    "    \n",
    "    # n = Number of nodes per community\n",
    "    # q = Number of communities\n",
    "    run_label = \"stdmerge-n32-q8-pout01.t00100\"\n",
    "    input_graph = f\"../data/cm4ai-community-detection-benchmark/{run_label}.graph\"\n",
    "    ground_truth_path = f\"../data/scm4ai-community-detection-benchmark/{run_label}.comms\"\n",
    "\n",
    "    edgelist = pd.read_csv(input_graph, sep=' ', names=[\"source\",\"target\"])\n",
    "    G = nx.from_pandas_edgelist(edgelist)\n",
    "    for edge in G.edges():\n",
    "        G[edge[0]][edge[1]]['weight'] = 1\n",
    "\n",
    "    gt_arr=[]\n",
    "    with open(ground_truth_path) as ground_truth_file:\n",
    "        for line in ground_truth_file:\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            fields = line.strip().split(\" \")\n",
    "            gt_arr.append(fields[1])\n",
    "elif args_dict[\"benchmark_gen\"] == 'football':\n",
    "\n",
    "    print(f\"Using benchmark graph generated by: Football\")    \n",
    "    \n",
    "    run_label = \"football\"\n",
    "    input_graph = f\"../data/cm4ai-community-detection-benchmark/football_adjacency_matrix.csv\"\n",
    "    input_graph_named = f\"../data/cm4ai-community-detection-benchmark/football.gml\"\n",
    "    ground_truth_path = f\"../data/cm4ai-community-detection-benchmark/football_labels.csv\"\n",
    "    ground_truth_named_path = f\"../data/cm4ai-community-detection-benchmark/football_labels_named.csv\"\n",
    "    adj_matrix = np.loadtxt(input_graph, delimiter=\",\", skiprows=1)\n",
    "    \n",
    "    # Create a graph from the adjacency matrix\n",
    "    G = nx.from_numpy_matrix(adj_matrix)\n",
    "\n",
    "    gt_arr=[]\n",
    "    with open(ground_truth_path) as ground_truth_file:\n",
    "        reader = csv.reader(ground_truth_file)\n",
    "        next(reader)  # Skip the header\n",
    "        for row in reader:\n",
    "            gt_arr.append(int(row[1]))\n",
    "            \n",
    "    gt_arr_named = []\n",
    "    with open(ground_truth_named_path, \"r\") as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader)  \n",
    "        for row in reader:\n",
    "            gt_arr_named.append(row[1]) \n",
    "    \n",
    "    # Read in the named graph to get the node names\n",
    "    G_named = nx.read_gml(input_graph_named)\n",
    "\n",
    "    # Extract the 'name' attribute for each node\n",
    "    node_data = [(node, data.get('name', '')) for node, data in G_named.nodes(data=True)]\n",
    "\n",
    "    # Save to a file\n",
    "    gt_node_names = []\n",
    "    for node_id, name in node_data:\n",
    "        gt_node_names.append(node_id)\n",
    "\n",
    "elif args_dict[\"benchmark_gen\"] == 'protein':\n",
    "\n",
    "    print(f\"Using benchmark graph generated by: Protein\")    \n",
    "\n",
    "    run_label = \"protein\"\n",
    "    edge_list_path = f\"../data/cm4ai-community-detection-benchmark/quantum_ppi_cutoff_0.002.id.edgelist.tsv\"\n",
    "    \n",
    "    # Create a graph from the edge list\n",
    "    G = nx.read_edgelist(edge_list_path, delimiter=\"\\t\", nodetype=int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(\n",
    "    G, \n",
    "    node_size=25,\n",
    "    width=0.5,\n",
    "    node_color=\"grey\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = nx.adjacency_matrix(G)\n",
    "print ('\\nAdjacency matrix:\\n', A.todense())\n",
    "\n",
    "if len(gt_arr) != 0:\n",
    "    num_parts = len(np.unique(gt_arr))\n",
    "else:\n",
    "    num_parts = 40\n",
    "\n",
    "num_blocks = num_parts \n",
    "num_nodes = nx.number_of_nodes(G)\n",
    "num_edges = nx.number_of_edges(G)\n",
    "print (f\"\\nQuantum Community Detection: Up to {num_parts} communities\")\n",
    "print (f\"Graph has {num_nodes} nodes and {num_edges} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta, gamma, GAMMA  = QCD.set_penalty_constant(num_nodes, num_blocks, args_dict[\"beta0\"], args_dict[\"gamma0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mtotal, modularity = QCD.build_mod(A, args_dict[\"threshold\"], num_edges)\n",
    "mtotal, modularity = QCD_optimized.build_mod_resolution(A, args_dict[\"threshold\"], num_edges, resolution=args_dict[\"resolution\"])\n",
    "\n",
    "print (\"\\nModularity matrix: \\n\", modularity)\n",
    "\n",
    "print (\"min value = \", modularity.min())\n",
    "print (\"max value = \", modularity.max())\n",
    "\n",
    "print (\"threshold = \", args_dict[\"threshold\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict['resolution']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled modularity, community penalty, sampling time\n",
    "\n",
    "# Higher resolution parameter\n",
    "# TODO: Didn't realize that I ran this when I added the 'resolution' parameter inside the modularity matrix function\n",
    "# need to test whether or not this line helps increase ARI or if we can just add resolution to the modularity matrix\n",
    "modularity_scaled = args_dict[\"resolution\"] * modularity\n",
    "\n",
    "# Q = QCD.makeQubo(G, modularity, beta, gamma, GAMMA, num_nodes, num_parts, num_blocks, threshold)\n",
    "Q = QCD_optimized.makeQubo(\n",
    "    modularity_scaled, \n",
    "    beta, \n",
    "    gamma, \n",
    "    GAMMA, \n",
    "    num_nodes, \n",
    "    num_parts, \n",
    "    num_blocks, \n",
    "    args_dict[\"threshold\"],\n",
    "    args_dict[\"community_penalty_factor\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}\n",
    "result['num_clusters'] = num_parts \n",
    "result['nodes'] = num_nodes\n",
    "result['edges'] = num_edges\n",
    "result['size'] = num_nodes * num_parts \n",
    "result['subqubo_size'] = args_dict[\"qsize\"]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run k-clustering with Hybrid/D-Wave using ocean\n",
    "ss = QCD.clusterHybrid(Q, num_parts, args_dict[\"qsize\"], run_label, run_profile, result)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process solution\n",
    "part_number = QCD.process_solution(ss, G, num_blocks, num_nodes, num_parts, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmetric = QCD.calcModularityMetric(mtotal, modularity, part_number)\n",
    "result['modularity_metric'] = mmetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw graph clusters and save .png\n",
    "GFU.showClusters(part_number, G, args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict['resolution']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write comms file \n",
    "GFU.write_partFile(\n",
    "    part_num=part_number, \n",
    "    Dim=num_nodes, \n",
    "    nparts=num_parts, \n",
    "    args_dict=args_dict\n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(gt_arr) != 0:\n",
    "    # Add partition ID as a node attribute\n",
    "    for node, cluster_id in part_number.items():\n",
    "        G.nodes[node]['cluster_id'] = cluster_id\n",
    "\n",
    "    # Add gt partition ID as attribute\n",
    "    for node in G.nodes:\n",
    "        G.nodes[node]['gt_cluster_id'] = gt_arr[node]\n",
    "        G.nodes[node]['gt_cluster_named'] = gt_arr_named[node]\n",
    "        G.nodes[node]['gt_team_name'] = gt_node_names[node]\n",
    "\n",
    "    # Save the graph as a NetworkX graph object\n",
    "    graph_path = os.path.join(args_dict['output_dir'], \"graph_with_clusters_good_ARI.graphml\")\n",
    "    nx.write_graphml(G, graph_path)\n",
    "else:\n",
    "    print(\"No ground truth available for this graph. Skipping graph saving.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange node and community IDs for metric calculations\n",
    "columns = [\"node_id\", \"comm_id\"]\n",
    "communities = []\n",
    "\n",
    "pred_arr=[]\n",
    "\n",
    "comm_file_name = f\"nparts_{num_parts}_resolution_{args_dict['resolution']}.txt\"\n",
    "comm_file_path = os.path.join(args_dict['output_dir'], comm_file_name)\n",
    "with open(comm_file_path) as comm_file:\n",
    "    i = 0\n",
    "    for line in comm_file:\n",
    "        i += 1\n",
    "        if i == 1:\n",
    "            continue\n",
    "        fields = line.strip().split(\"  \")\n",
    "        communities.append(fields)\n",
    "        pred_arr.append(fields[1])\n",
    "\n",
    "pred_arr = [int(x) for x in pred_arr]\n",
    "pred_arr[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modularity = nx.community.modularity(G, pred_arr)\n",
    "print(\"Number of communities (DWave):\", len(np.unique(pred_arr)))\n",
    "\n",
    "print(f\"Modularity: \\t{round(result['modularity_metric'],4)}\")\n",
    "\n",
    "if len(gt_arr) != 0:\n",
    "    # Calculate adjusted mutual information and adjusted rand index\n",
    "    result['ari_score'] = adjusted_rand_score(gt_arr, pred_arr)\n",
    "    result['ami_score'] = adjusted_mutual_info_score(gt_arr,pred_arr)\n",
    "\n",
    "    print(f\"ARI: \\t\\t{round(result['ari_score'],4)}\")\n",
    "    print(f\"AMI: \\t\\t{round(result['ami_score'],4)}\")\n",
    "else:\n",
    "    print(\"No ground truth available for this graph. Skipping ARI and AMI calculations.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Louvain Community Detection ---\n",
    "# Perform Louvain community detection\n",
    "louvain_partition = community_louvain.best_partition(G, resolution=1.5)\n",
    "num_communities_louvain = len(set(louvain_partition.values()))\n",
    "print(\"Number of communities (Louvain):\", num_communities_louvain)\n",
    "\n",
    "# --- Calculate Modularity ---\n",
    "modularity_score = community_louvain.modularity(louvain_partition, G)\n",
    "print(f\"Modularity: \\t{round(modularity_score,4)}\")\n",
    "\n",
    "louvain_pred_arr = [louvain_partition[node] for node in G.nodes]\n",
    "\n",
    "if len(gt_arr) != 0:\n",
    "    # --- Calculate ARI and AMI ---\n",
    "    ari_score = adjusted_rand_score(gt_arr, louvain_pred_arr)\n",
    "    ami_score = adjusted_mutual_info_score(gt_arr, louvain_pred_arr)\n",
    "    print(f\"ARI: \\t\\t{round(ari_score,4)}\")\n",
    "    print(f\"AMI: \\t\\t{round(ami_score,4)}\")\n",
    "else:\n",
    "    print(\"No ground truth available for this graph. Skipping Louvain ARI and AMI calculations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the modularity score for the ground truth \n",
    "if len(gt_arr) != 0:\n",
    "    # Convert gt_arr to a dictionary format for modularity calculation\n",
    "    gt_partition = {node: community_id for node, community_id in enumerate(gt_arr)}\n",
    "\n",
    "    # Calculate modularity using community_louvain.modularity\n",
    "    modularity_score = community_louvain.modularity(gt_partition, G)\n",
    "    print(f\"Ground Truth Modularity: {round(modularity_score,4)}\")\n",
    "else:\n",
    "    print(\"No ground truth available for this graph. Skipping ground truth modularity calculation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cm4ai-quantum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
